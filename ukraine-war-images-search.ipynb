{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ukraine War Images Search \n### (Powered by CLIP)\n\n## Overview\n\nThis example implements search for Ukraine War Images\n\n* You'll be able to use **natural text** queries for image search\n* The search will work even **when the image has no such info in the captions** or metadata\n* The search will work even when you make a ~typpo~ typo\n\n**Example natural text queries and results**\n\n\n[![WcUknj.md.png](https://iili.io/WcUknj.md.png)](https://freeimage.host/i/WcUknj)\n\n\nPossibilities are endless. As long as the data has the images matching your natural text, you get sruprisingly accurate results.","metadata":{"id":"fbe6c29e-55fe-4114-95de-5efa46cbb90c"}},{"cell_type":"markdown","source":"### Install prerequisites","metadata":{"id":"49730b68-de7f-44d3-924f-f38adef6786d"}},{"cell_type":"code","source":"!python --version\n!pip install -q clip-client \n!pip install -q ipywidgets # look nice in notebook","metadata":{"id":"367774ed-0f0a-41e0-97eb-0b5b49bb5033","outputId":"112ea326-69a8-4342-dff4-4b4c9d80fe1c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some nice to have dependencies","metadata":{"id":"a24b9d77-0292-414a-ab69-7c831323dc73"}},{"cell_type":"code","source":"# Disables warnings otherwise screen gets flashy\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Install matplotlib for sprite render\n!pip install -q matplotlib","metadata":{"id":"54d45b23-f970-4494-9c50-d06b1ba9bbc5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download the dataset - \"Ukraine War Images\"\n","metadata":{"id":"zVCQVamk4343"}},{"cell_type":"markdown","source":"For this example, we are going to use the \"Ukraine War Images\" dataset available at https://www.kaggle.com/datasets/mathurinache/ukraine-war-images. At the time of coding this example(Apr 2022), this dataset has 2000+ images. ","metadata":{"id":"Rz5TZvp68EAi"}},{"cell_type":"markdown","source":"Here, we are downloading dataset from Kaggle.\n\n\n**A. If you are not using this notebook on Kaggle Notebooks**\n\nWe will need [Kaggle API credentials](https://github.com/Kaggle/kaggle-api#api-credentials).\n\nDownload the credentials(`kaggle.json`) from your Kaggle account and upload it to this Google Colab's folder. Once you're done with that, run the following code.","metadata":{}},{"cell_type":"code","source":"#!pip install kaggle # Install kaggle library\n#!mkdir ~/.kaggle # Make a directory for kaggle configs/credentials\n#!cp kaggle.json ~/.kaggle/ # Copy kaggle api credentials in .kaggle\n#!chmod 600 ~/.kaggle/kaggle.json # Permissions","metadata":{"id":"ZjdBXYA44_ME","outputId":"cf58bb93-eb37-4051-98f7-49b425d0d9d8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download the data using `kaggle` package commands","metadata":{}},{"cell_type":"code","source":"# !kaggle datasets download mathurinache/ukraine-war-images\n# !unzip ukraine-war-images","metadata":{"id":"dtHUSUmz8Nrk","outputId":"40194231-a187-49eb-b036-a04a8ff22e34","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**B. If you're using this notebook on Kaggle Notebooks**\n\nNothing else to do here. Just reference the data at `../input/ukraine-war-images/` folder using (`+Add Data` button).\n\nIf you face any issues, [refer this video](https://youtu.be/VaJEK6fycwM).","metadata":{}},{"cell_type":"markdown","source":"### Connect to CLIP server\n\nCLIP server is the \"backend of the search\", responsible for encoding the image data and matching them with the query. It uses the pretrained CLIP Neural Network model.\n\nI'm going to use a hosted demo instance of CLIP server, so all you need to do is run the following code.","metadata":{"id":"b3be3478-bc75-4be6-9fee-2709e2ce6e10"}},{"cell_type":"code","source":"from clip_client import Client\n\nhost = \"grpc://demo-cas.jina.ai:51000\"\n\nc = Client(host)","metadata":{"id":"bb4e87bf-2a16-4098-96db-10a5273abb94","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Alternatively, you could run your own [CLIP server](https://clip-as-service.jina.ai/user-guides/server/) by simply running `python -m clip_server` and replacing the `demo-cas.jina.ai:51000` with your server's IP","metadata":{"id":"gMzySXzOk3dK"}},{"cell_type":"markdown","source":"## Load the image data in DocumentArray\nDocumentArray is a data structure for unstructured data such as images. It comes with the `clip-client` and will be helpful in implementing a scalable search solution in just few lines of code.","metadata":{"id":"yFP6RMj6AeZy"}},{"cell_type":"code","source":"from docarray import DocumentArray\nimg_da = DocumentArray.from_files('../input/ukraine-war-images/ukraine_war/*.png')\n# Let's see how does our data look like\nimg_da.plot_image_sprites()","metadata":{"id":"H-33zNFPAhiJ","outputId":"cb129434-5aac-4493-b2c5-ff43930fdc0f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encode the image data\nBefore we implement search, we need to encode the image data. It means creating vector representations of the images(also known as embeddings).","metadata":{"id":"Xi1cLrIGZopn"}},{"cell_type":"code","source":"try:\n    # If we already have saved the encoded data on cloud, let's just get it directly from there\n    da = DocumentArray.pull('saved_ukraine_war_image_embeddings_on_cloud', show_progress=True, local_cache=True)\nexcept BaseException:\n    # Otherwise let's encode the data\n    da = c.encode(img_da, batch_size=8, show_progress=True)\n    # If you want to reuse the embeddings later, push the embeddings to cloud. \n    da.push('saved_ukraine_war_image_embeddings_on_cloud', show_progress=True)\n    # Note: cloud stores data only for 1 week\n\n# Show the embeddings data sprites\nda.plot_image_sprites()","metadata":{"id":"4EBNwp08VSG1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample search","metadata":{"id":"456b432a-7ed8-47d7-ad23-42683ad3715a"}},{"cell_type":"code","source":"input_texts = [\n    \"tanks\",\n    \"rockets\",\n    \"destroyed vehicles\",\n    \"destroyed buildings\",\n    \"fight in the snow\",\n    \"different vehicles used in the war\",\n    \"different weapons used in the war\",\n    \"two tanks in front of each other\",\n    \"man holding weapon\"\n]\n\nfor txt in input_texts:\n    print(txt)\n    vec = c.encode([txt])\n    r = da.find(query=vec, limit=4)\n    r.plot_image_sprites()\n    print(\"-----\")","metadata":{"id":"74f97805-5867-4bc1-9670-be05495b56d2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nWe implemented natural semantic text to image search for 2000+ images of \"Ukraine War Images Data\". We used `CLIP` pre trained Neural Network model to do that. [`CLIP-as-service`](https://github.com/jina-ai/clip-as-service) made it easy to implement a scalable solution. This technique of search is called Neural Search.\n\n\n## Resources\n* [What is Neural Search](https://www.kdnuggets.com/2021/05/what-neural-search.html)\n* [Clip-as-service python package](https://github.com/jina-ai/clip-as-service)\n* [DocArray - Data structure for unstructured data](https://docarray.jina.ai)","metadata":{"id":"56c49479-bda6-4e6c-9974-c0c89e1b70d3"}}]}